{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network metrics\n",
    "\n",
    "This notebook computes different network centrality measures (degree, pagerank, etc.) for candidates nodes. The network was build using the retweet information: if a twitter user A retweets user B, A -> B (A points to B), a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import new_class\n",
    "import pandas as pd\n",
    "import pyathena\n",
    "import pymongo\n",
    "from pymongo.errors import BulkWriteError\n",
    "import os\n",
    "import dotenv\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import logging\n",
    "import gc\n",
    "import plotly.io as pio\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] - %(name)s - %(funcName)s - %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "mongo_client = pymongo.MongoClient(os.environ[\"MONGODB_URL\"])\n",
    "twitter_db = mongo_client.TwitterConstituyenteDB\n",
    "\n",
    "conn = pyathena.connect(s3_staging_dir=os.environ[\"AWS_ATHENA_S3_STAGING_DIR\"], region_name=os.environ[\"AWS_REGION\"])\n",
    "candidates_df = pd.read_sql(\"\"\"SELECT * FROM \"twitter-constituyente\".\"constituyentes\";\"\"\", conn)\n",
    "candidates_ids = candidates_df[\"user__id_str\"].dropna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute network metrics in 7 day window and save to mongo\n",
    "def compute_graph_metrics(graph):\n",
    "    for func, col in tqdm.tqdm([\n",
    "        (lambda g: {node: value for node, value in g.degree()}, \"degree\"),\n",
    "        (lambda g: {node: value for node, value in g.in_degree()}, \"in_degree\"),\n",
    "        (lambda g: {node: value for node, value in g.out_degree()}, \"out_degree\"),\n",
    "        (nx.degree_centrality, \"degree_centrality\"),\n",
    "        (partial(nx.eigenvector_centrality, max_iter=1000), \"eigenvector_centrality\"),\n",
    "        (nx.in_degree_centrality, \"in_degree_centrality\"),\n",
    "        (nx.out_degree_centrality, \"out_degree_centrality\"),\n",
    "        # (nx.closeness_centrality, \"closeness\"),\n",
    "        # (nx.betweenness_centrality, \"betweenness\"),\n",
    "        (nx.pagerank, \"pagerank\"),\n",
    "        # (nx.katz_centrality_numpy, \"katz\"),\n",
    "    ]):\n",
    "        nx.set_node_attributes(\n",
    "            graph, func(graph), col\n",
    "        )\n",
    "\n",
    "def bulk_write_to_mongo(collection, data):\n",
    "    to_insert = len(data)\n",
    "    try:\n",
    "        if to_insert > 0:\n",
    "            collection.insert_many(data, ordered=False)\n",
    "        return to_insert, 0\n",
    "    except BulkWriteError as e:\n",
    "        log.error(\"BulkWriteError\")\n",
    "        inserted = e.details[\"nInserted\"]\n",
    "        return inserted, to_insert - inserted\n",
    "\n",
    "\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date(2021, 5, 14)\n",
    "delta = datetime.timedelta(days=1)\n",
    "\n",
    "start_window = start_date\n",
    "end_window = start_window + 6 * delta\n",
    "window_df = pd.read_sql(f\"\"\"\n",
    "        SELECT * FROM \"twitter-constituyente\".\"daily_graph\"\n",
    "        WHERE (date >= DATE('{(start_window-delta).strftime(\"%Y-%m-%d\")}')) AND (date <= DATE('{(end_window-delta).strftime(\"%Y-%m-%d\")}'));\"\"\", conn) \n",
    "\n",
    "while start_date <= end_date:\n",
    "    start_window = start_date\n",
    "    end_window = start_window + 6 * delta\n",
    "    # window_df = df[(df[\"date\"]>=start_window) & (df[\"date\"]<=end_window)]\n",
    "    \n",
    "    missing_day_df = pd.read_sql(f\"\"\"\n",
    "        SELECT * FROM \"twitter-constituyente\".\"daily_graph\"\n",
    "        WHERE date = DATE('{end_window.strftime(\"%Y-%m-%d\")}');\"\"\", conn) \n",
    "    window_df = window_df[window_df[\"date\"]>=start_window].append(missing_day_df, ignore_index=True)\n",
    "\n",
    "    log.info(f\"Window: {start_window} - {end_window}, Length: {len(window_df)}\")\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_weighted_edges_from((window_df\n",
    "                .groupby([\"source\", \"target\"])\n",
    "                .agg({\"count\": \"sum\"})\n",
    "                .reset_index()\n",
    "                .itertuples(index=False, name=None)))\n",
    "    compute_graph_metrics(graph)\n",
    "    # del window_df\n",
    "    # gc.collect()\n",
    "    graph_data = pd.DataFrame.from_dict(dict(graph.nodes(data=True)), orient='index')\n",
    "    graph_data[\"window_start\"] = datetime.datetime.combine(start_window, datetime.datetime.min.time())\n",
    "    graph_data[\"window_end\"] = datetime.datetime.combine(end_window, datetime.datetime.min.time()) \n",
    "    \n",
    "    inserted, not_inserted = bulk_write_to_mongo(\n",
    "        twitter_db.network_timeseries_7dayswindow_fullbase, \n",
    "        graph_data.reset_index().rename(columns={\"index\": \"user__id_str\"}).to_dict('records')\n",
    "    )\n",
    "    log.info(f\"Inserted: {inserted}, Not inserted: {not_inserted}\")\n",
    "    start_date += delta"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
